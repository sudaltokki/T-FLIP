<!-- Template for PROJECT REPORT of CapstoneDesign 2024-2H, initially written by khyoo -->
<!-- 본 파일은 2024년도 컴공 졸업프로젝트의 <1차보고서> 작성을 위한 기본 양식입니다. -->
<!-- 아래에 "*"..."*" 표시는 italic체로 출력하기 위해서 사용한 것입니다. -->
<!-- "내용"에 해당하는 부분을 지우고, 여러분 과제의 내용을 작성해 주세요. -->

# Team-Info
| (1) 과제명 | *T-FLIP: Lightweighting CLIP for Face Anti-Spoofing via Knowledge Distillation*
|:---  |---  |
| (2) 팀 번호 / 팀 이름 | *04-티라노* |
| (3) 팀원 역할 분담 |  박지원(2076169) : 리더, *AI 모델 개발*   <br> 류이정(2176129) : 팀원, *AI 모델 개발*   <br> 소예림(2071028) : 팀원, *AI 모델 개발*  |
| (4) 팀 지도교수 | 심재형 교수님 |
| (5) 팀 멘토 | 최종원 / 조교수 / 중앙대학교 첨단영상대학원 영상학과 |
| (6) 과제 분류 | 연구 과제 |
| (7) 과제 키워드(keywords) | CLIP, Face Anti Spoofing(FAS), 경량화, knowledge distillation |
| (8) 과제 내용 요약 | FLIP(Face Anti-Spoofing with Language-Image Pretraining)은 멀티모달 모델인 CLIP(Contrastive Language-Image Pretraining)을 Face Anti-Spoofing(FAS) 태스크에 최적화한 모델이다. FLIP은 위조 얼굴을 탐지하는 FAS 태스크에서 다른 모델들에 비해 우수한 성능을 보여준다. 그러나 거대한 모델 구조로 인해 추론에 많은 비용이 발생하며, 엣지 디바이스에 탑재하는 데에 어려움이 있다. 이에 따라, Knowledge Distillation(지식 증류) 방법을 사용하여 FLIP의 우수한 성능을 유지하면서도 더 빠르게 추론할 수 있는 경량화 모델을 개발하고자 한다. |
<br>

# Project-Summary
| 항목 | 내용 |
|:---  |---  |
| (1) 문제 정의 | Face Anti-Spoofing(FAS)는 보안이 중요한 응용 프로그램에서 사용되는 얼굴 인식 시스템의 필수 요소이다. 현대 사회에서는 휴대폰 잠금 해제부터 공항 탑승 게이트까지 FAS 기술이 우리 생활에 밀접하게 사용되고 있다. FAS는 보안과 직결되는 기술이지만 더불어 매우 흔하고, 빈번하게 사용되는 기술이기 때문에 우리는 높은 성능 뿐 만 아니라 빠른 추론 속도의 필요성 또한 절감했다. 어디서든 보안이 필요한 곳이라면 컴퓨팅 능력과 자원이 한정된 디바이스에도 좋은 성능을 보일 수 있어야 한다. <br><br> 현재 연구에 사용되는 FLIP(Face Anti-Spoofing with Language-Image Pretraining) 모델은 CLIP(Contrastive Language-Image Pretraining) 기반의 멀티모달 모델로서, 이미지와 텍스트 간의 관계를 이해하는 데에 효과적이고 위조 얼굴 탐지(FAS) 태스크에서 뛰어난 성능을 보여준다. 그러나 169,568,513개의 파라미터를 가지고 있는 매우 크고 복잡한 모델 구조로 인해 추론 속도가 느리며 또 많은 비용이 발생한다. 이러한 이유로 FLIP 모델은 실시간 처리가 중요한 엣지 디바이스에 적합하지 않으며, 모바일 기기나 저사양 디바이스에서의 활용이 어렵다. <br><br> 따라서 Target Customer는 엣지 컴퓨팅 환경에서 보안이 중요한 애플리케이션을 사용하는 기업 및 개발자들이다. 이러한 고객들이 직면한 문제점은 다음과 같다:<br><br>- **엣지 디바이스 성능 제한**: 위조 얼굴 탐지와 같은 고성능 태스크는 엣지 디바이스에서 수행하기에 자원과 처리 속도 측면에서 많은 제약을 받는다.<br> - **실시간 위조 탐지 필요성**: 보안성 향상을 위해 위조 얼굴을 실시간으로 탐지하는 기능이 필수적이지만, FLIP 모델의 큰 구조로 인해 처리 속도가 느려 실시간 추론에 적합하지 않다.<br> - **추론 비용 문제**: 큰 모델을 클라우드 기반으로 운영할 경우 비용이 많이 들고, 이를 엣지 디바이스에서 수행할 수 있는 경량화된 대안이 필요하다.|
| (2) 기존연구와의 비교 | 본 과제는 FLIP 모델을 기반으로 FAS 작업에서의 성능과 효율성을 모두 향상시킨 모델 T-FLIP을 개발하는 것을 목표로 한다. FLIP을 경량화 하고자 하는 본 과제를 기존 연구들과 비교하여 FAS 과제에서 해당 FLIP 모델을 선택한 이유와, 이 모델을 경량화함으로써 갖는 장점을 설명해보고자 한다. <br><br> 1) 비전 트랜스포머(ViT) 모델을 사용한 FAS 연구[1][2] : <br> 최근 ViT 모델은 이미지 패치 간의 장거리 의존성을 포착하는 능력 덕분에 FAS 작업에 효과적인 것으로 나타났다. 그러나 해당 연구에는 두 가지 한계점이 있다. 첫째, 이미지 데이터만을 사용하여 학습하기 때문에 제한된 학습 데이터를 사용할 때 일반화 능력을 제한한다. 둘째, 사전 학습된 가중치를 미세 조정하기 위해 추가적인 네트워크 수정이나 도메인 레이블과 같은 추가 정보 구성 작업을 필요로 한다. 반면, T-FLIP에서 선택한 FLIP 모델은 이미지와 텍스트를 결합한 멀티모달 모델 사전학습 가중치로 ViT를 초기화하는 과정을 거쳐 더욱 향상된 FAS 성능을 보여준다. 또한, 이미지 표현을 클래스를 설명하는 텍스트 집합과 정렬하기 때문에, 데이터가 적은 상황에서도 FAS의 일반화 성능이 향상된다. <br><br> 2) CLIP(Contrastive Language-Image Pretraining) Knowledge distillation 연구[3][4][5] : <br> 국내외에서는 이미지와 텍스트를 동시에 이해하는 멀티모달 인공지능 모델의 개발이 활발히 진행되고 있다. FLIP 모델은 멀티모달 모델인 CLIP을 FAS 태스크에 최적화시킨 모델로서 CLIP의 아키텍쳐에 그 기반을 두고 있다. CLIP 모델을 경량화하고자 했던 이전 연구를 살펴보면, Tiny-CLIP, CLIP-KD, RKD 등의 연구는 CLIP과 유사한 성능을 유지하면서 파라미터 수를 효과적으로 감소시켜 다양한 knowledge distillation 기법의 효용성을 입증했다. 그러나 CLIP은 FAS 작업에 최적화된 모델이 아닐 뿐 더러, CLIP과 유사한 구조를 가진 FLIP에 대해서는 아직 경량화 시도가 진행된 바가 없다. 따라서 본 프로젝트에서는 FAS 과제에 최적화된 모델의 경량화를 해내고자 FLIP에 knowledge distillation을 적용하였다. <br><br> 위와 같은 이유로 본 연구는 기존에 시도되지 않았던 FLIP 모델을 knowledge distillation으로 경량화하는 독창적인 시도를 통해 T-FLIP이 비교적 우수한 FAS 성능과 빠른 추론 속도를 가질 것이라 예상하며, 나아가 실제 환경에서 FAS 태스크의 적용 가능성을 높일 것이라 기대한다. <br><br> 출처 : <br>  [1]  Anjith George and Se ́bastien Marcel. On the effectiveness of vision transformers for zero-shot face anti-spoofing. In 2021 IEEE International Joint Conference on Biometrics (IJCB), pages 1–8. IEEE, 2021. <br> [2] Hsin-PingHuang,DeqingSun,YaojieLiu,Wen-ShengChu, Taihong Xiao, Jinwei Yuan, Hartwig Adam, and Ming- Hsuan Yang. Adaptive transformers for robust few-shot cross-domain face anti-spoofing. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XIII, pages 37–54. Springer, 2022. <br> [3] Wu, Kan, et al. "Tinyclip: Clip distillation via affinity mimicking and weight inheritance." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023. <br> [4] Yang, Chuanguang, et al. "CLIP-KD: An Empirical Study of Distilling CLIP Models." arXiv preprint arXiv:2307.12732, 2023. <br> [5] Park, Wonpyo, et al. "Relational knowledge distillation." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.|
| (3) 제안 내용 | 1. FLIP 모델 경량화의 첫번째 시도 : 딥러닝 모델의 경량화 연구는 다양한 분야에서 활발히 진행되고 있지만, 최근 얼굴 위조 탐지(Face Anti-spoofing, FAS) 분야에서 뛰어난 성능을 보인 FLIP(FLIP: Cross-domain Face Anti-spoofing with Language Guidance) 모델에 대한 경량화 연구는 아직 이루어지지 않았다는 점이 주목할 만하다. FLIP 모델은 그 우수한 성능에도 불구하고, 약 1억개 이상의 대규모 파라미터와 느린 추론 속도로 인해 실제 응용에 제약이 있다. 본 연구 프로젝트는 knowledge distillation(KD)를 도입하여 FAS의 추론 속도를 향상시키고 FLIP 모델의 한계를 극복하고자 한다. <br><br> 2. 다양한 KD 방법론의 비교 : 본 연구는 FLIP 모델의 고유한 특성을 최대한 활용하기 위해 다양한 KD 기법을 적용하며 최종적으로 FLIP을 가장 잘 경량화하는 KD 방법론을 찾아가는 것에 의의가 있다. 특히, 단순한 logit 기반의 distillation을 넘어서 feature distillation과 relational knowledge distillation(RKD)의 장점을 결합한 접근 방식을 채택할 예정이다. 이를 통해 목표 성능이었던 90%이상의 AUC, 2배 이사의 추론속도 향상을 달성해냈으며, 추가적인 성능 향상을 위해 새로운 방법론을 탐색 중이다. <br><br> 3. FLIP에 적합한 KD 방식 채택 : 다양한 KD 방법론을 적용해본 경험에 따르면, 어텐션 맵에 중점을 둔 distillation이 FLIP 모델의 성능을 경량 모델에 효과적으로 전달할 수 있을 것이라 기대한다. 이는 학생모델로 하여금 교사모델이 특정한 시각적 특징에 집중하는 방식을 학습하는 데 도움이 될 것이고 최종적으로 T-FLIP의 성능 향상을 이뤄낼 수 있을 것이다. <br><br> 이러한 다면적 접근은 FLIP 모델의 핵심 강점인 언어 가이던스와 크로스 도메인 능력을 경량 모델에 더욱 효과적으로 전달할 수 있을 것으로 기대된다.<br> 즉, 본 연구는 계산 효율성을 크게 향상시키면서도 원래 FLIP 모델에 근접한 성능을 가진 경량화된 모델 T-FLIP을 개발하는 것을 목표로 한다.|
| (4) 기대효과 및 의의 | 본 연구 모델 T-FLIP의 목표는 FAS 기술을 사용하고자 하는 모든 기술 및 기업을 목표 사용자로 하여, FLIP (Face Anti-Spoofing with Language-Image Pretraining) 모델의 추론 속도를 향상시켜 보다 효율적이고 실용적인 사용 환경을 조성하는 것이다. <br><br> 구체적으로는 다음과 같은 목표를 설정하였다:<br> 1. FLIP의 추론 속도를 기존보다 최소 2배 이상 향상시키는 것을 목표로 한다. 이를 통해 FLIP 모델의 사용성을 향상시키고, 실제 환경에서의 빠른 응답이 가능하도록 한다.<br> 2. T-FLIP 모델에게도 FLIP과 유사한 성능을 제공하여 모델의 크기를 줄이고 자원을 효율적으로 활용할 수 있도록 한다. 이를 통해 모바일 및 임베디드 시스템에서도 FLIP 기술을 적용할 수 있는 환경을 조성한다.<br> 3. 추론 속도의 향상이 모델의 정확도에 부정적인 영향을 미치지 않도록 한다. 즉, 정확도는 90% 이상을 유지할 수 있도록 하며 속도와 정확도 간의 균형을 유지하여 실제 사용 시에도 뛰어난 성능을 제공한다.<br> 4. FLIP 모델의 개선을 통해 기업들은 보다 빠르고 정확한 FAS 인공지능 서비스를 제공할 수 있게 되어 산업 혁신과 경쟁력 강화에 기여할 것으로 기대된다.<br><br> T-FLIP을 통해 위 목표를 달성함으로써, FAS 태스크의 보안성을 높여 기술 사용자들의 불안함을 해소하고, FLIP 기반의 다중 모달 인공지능 모델을 보다 효율적으로 개발하고 실제 응용 환경에서의 적용 가능성을 높이고자 한다.|
| (5) 주요 기능 리스트 | 1. FD(Feature Distillation): <br> knowledge distillation 방법 중 교사 모델(teacher model)에서 추출한 피처(feature)를 학생 모델(student model)로 증류하는 방법이다. 교사 모델은 학습 과정에서 여러 단계의 중간 피처를 생성하는데, 이 피처들은 입력 데이터의 특정 속성을 표현하는 중요한 정보이다. FD의 핵심은 교사 모델의 여러 레이어에서 추출된 피처맵을 학생 모델로 전이시켜, 교사 모델이 학습한 중요한 정보(피처)를 학생 모델이 학습하도록 돕는 것이다. 이는 단순히 출력 logits를 맞추는 방식보다 더 세밀하게 정보를 전달하는 방법으로, 학생 모델이 더 효율적으로 학습할 수 있도록 한다. <br><br> 2. RKD(Relational Knowledge Distillation): <br> Relational Knowledge Distillation(RKD)은 데이터 간의 관계성(relations)을 기반으로 지식을 증류하는 방법이다. 일반적인 피처 증류는 교사 모델과 학생 모델 간의 개별 데이터에 대한 매핑에 초점을 맞추는 반면, RKD는 여러 데이터 간의 상호작용이나 관계성을 학습하고 그 관계성을 학생 모델로 전이시키는 것을 목표로 한다. 이 방법론은 교사 모델이 학습한 데이터 간의 관계성(데이터 포인트 간의 거리, 상대적 위치, 유사성)을 학생 모델이 학습함으로써, 데이터 간의 구조적 정보를 효과적으로 전달한다. 특히 거리(distances)와 각도(angles)를 사용하여 데이터 간의 상대적 관계를 학습한다. 이를 통해 학생 모델이 데이터의 구조적 특성을 보존할 수 있도록 유도한다. <br><br> AFD(Attention-based Feature Distillation(AFD): <br> AFD는 Attention 메커니즘을 활용한 지식 증류 방법으로, Show, Attend and Distill 논문에서 제안된 방식이다. 이 방식에서는 교사 모델이 데이터에 대해 주목하는 부분(Attention)을 학생 모델에 전이하여, 학생 모델이 중요한 피처에 집중할 수 있도록 학습을 진행한다. Attention 메커니즘은 신경망 모델이 입력 데이터의 특정 부분에 대해 더 많은 가중치를 부여하는 기법으로, 특히 비전 및 NLP 태스크에서 중요한 정보를 선택적으로 학습하는 데 유용하다. AFD는 이 Attention 메커니즘을 활용하여, 교사 모델의 Attention 지도(attention map)를 학생 모델에 전이시키는 방식을 제안한다. 이는 교사 모델이 데이터의 어떤 부분에 집중하는지에 대한 정보를 학생 모델에 전달하여, 학생 모델이 중요한 피처를 놓치지 않고 학습할 수 있도록 한다.|

<br>
 
# Project-Design & Implementation
| 항목 | 내용 |
|:---  |---  |
| (1) 요구사항 정의 | <img width="1007" alt="requirements" src="https://github.com/user-attachments/assets/2dd59c44-8c02-401f-bf7a-77d478e8e590"> <br> 우선 데이터셋 관련한 요구사항을 설명하자면, 모델 훈련과 모델 성능 검증을 위한 데이터셋이 필요하다. 데이터셋은 잘 알려져있는 FAS 데이터셋에 해당하는 MSU-MFSD, CASIA-MFSD, Replay-Attack, OULU-NPU로 결정했다. 모델을 훈련시키고 성능을 평가할 때는 이 중 임의로 세개를 선택하여 학습 데이터셋으로 사용하고 나머지 한 개를 테스트 데이터셋으로 사용한다. <br> T-FLIP의 모델 요구사항에는 처리시간 및 파라미터 수 감소, 목표 정확도, 장비 사양 등이 있다. 우선 추론을 진행하는 데에 소요되는 처리시간과 파라미터 수를 감소시키는 것이 프로젝트의 목표이므로, 처리 시간을 두배 빠르게 만든다는 관련 요구사항에 따라 모델을 개발하고 있다. 다음으로 목표 정확도 관련 요구사항은 90% 이상을 유지하는 것이다. Knowledge distillation을 적용하되 작은 모델에서도 좋은 성능을 낼 수 있도록 최소 90% 이상의 정확도를 가질 수 있도록 모델을 개발한다. 추가적으로 모델을 훈련 시킬 때 필요한 장비인 GPU 서버는 NVIDIA GeForce RTX 4090로, 프로젝트 지도교수님인 심재형 교수님의 지원을 받아 사용하고 있다. |
| (2) 전체 시스템 구성 | <img width="965" alt="architecture" src="https://github.com/user-attachments/assets/b8d8fc44-bf01-437b-9677-cf132464238d"> 위 그림은 T-FLIP의 전체 시스템 구조이다. FLIP 모델을 교사모델로 삼아 경량화하고자 하였으며, FLIP git( https://github.com/koushiksrivats/FLIP.git )을 통해 코드를 받아 경량화 코드 개발을 진행했다. 데이터로는 FAS 데이터로 자주 사용되는 MSU-MFSD, CASIA-MFSD, Replay-Attack, OULU-NPU의 4개 데이터를 사용한다. MSU-MFSD는 실제 얼굴과 위변조 얼굴에 대한 35명의 비디오를 녹화한 데이터로 총 280개의 비디오를 가지고 있다. CASIA-MFSD는 인당 12개씩, 50명의 비디오를 서로 다른 해상도와 조명 조건 아래에서 녹화한 데이터셋이다. Replay-Attack는 50명의 비디오, 총 1300개의 비디오로 구성되어 있으며 실제 얼굴 외에 얼굴 사진 및 비디오를 녹화한 영상들로 이루어져 있다. 마지막으로 OULU-NPU는 조명조건과 배경장면이 다른 4950개의 실제 얼굴 및 위변조 얼굴 비디오로 구성되어 있다. 위 데이터셋 중 훈련 단계에서는 3개의 데이터셋만 활용하고, 테스트 단계에서는 학습에 사용하지 않은 1개의 데이터셋으로 평가하여 새로운 도메인에서도 효과적으로 대응할 수 있는지를 검증한다. 이를 통해 T-FLIP의 일반화 능력을 평가한다. <br><br> T-FLIP은 두 가지 방식으로 학습한다. 첫째, 3개의 데이터셋을 직접 학습하는데, 이 과정에서는 3개의 데이터셋을 직접 학습하여 얼굴 위조 탐지에 필요한 특징들을 학습한다. 둘째, 지식증류를 통해 교사 모델 FLIP으로부터 중요한 지식을 전달받는다. 이를 통해 작고 효율적인 모델이면서도 FLIP의 뛰어난 성능을 최대한 유지할 수 있다. 아래에서 T-FLIP 구조의 주요 모듈인 FLIP과 KD 방법론들에 대해 더 자세히 설명하고 있다. <br><br> T-FLIP의 훈련 과정에서 쓰이는 모듈은 다음 아래 두 그림과 같다. ![structure (1)_page-0001](https://github.com/user-attachments/assets/e93e5429-d61a-41e8-9fb9-0a5e96a56957) 먼저 위의 그림은 경량화 모델을 훈련시키는 데에 쓰이는 모듈이다. 먼저 simCLR Loss는 동일한 이미지에서 두개의 뷰를 생성하고, 두 변환된 이미지의 특징을 이미지 인코더를 통해 추출한 후, 비선형 투영 네트워크를 사용하여 투영된 특징에 대조학습을 적용한다. 이 과정을 통해 모델은 이미지 변환에 관계없이 일관된 특징을 학습하게 된다. 결론적으로, SimCLR loss는 변환된 두 이미지 뷰가 서로 가까운 임베딩 공간에 위치하도록 하여, 일반화 성능을 향상시킨다. <br><br> CE Loss는 Image Encoder와 Text Encoder의 대조학습의 결과로 나타난 클래스의 확률 분포가 실제 정답과 어느정도 차이가 나는지 계산할 때 사용된다. 마지막으로 MSE loss는 이미지-텍스트 간의 유사성 점수를 계산할 때 사용된다. 동일한 이미지-텍스트 쌍이 일관된 유사성 점수를 가지도록 강제하게 된다. ![structure (1)_page-0002](https://github.com/user-attachments/assets/fe7d807a-8b9b-4ab6-9f44-fa10ffb4c8c3) <br> 이 이미지는 경량화 모델 (student model)이 teacher 모델로부터 지식을 전달 받는 데에 사용되는 모듈이다. Teacher 모델이 데이터에 대해 주목하는 부분을 학생 모델에게 전달하여 학생 모델이 중요한 피처에 집중할 수 있도록 학습을 진행한다.|  
| (3) 주요엔진 및 기능설계 | T-FLIP의 핵심 모듈은 위변조 여부를 판단하는 FAS task와, 지식 증류이다. 각 모듈은 다음과 같은 역할을 수행한다.<br> 1.	FLIP의 task loss 모듈: T-FLIP은 3개의 데이터셋을 사용하여 직접 얼굴 위조 탐지에 필요한 특징을 학습한다. 이 과정에서는 여러 데이터셋으로부터 다양한 도메인 특징을 학습하여 모델이 넓은 범위의 도메인에 대해 강인한 일반화 성능을 가지도록 한다. <br> 2.	지식 증류 모듈: 지식 증류는 T-FLIP에서 가장 핵심적인 역할을 한다. 이 모듈은 교사 모델인 FLIP 모델로부터 중요한 지식을 T-FLIP에 전달하는 역할을 한다. 지식 증류 기법을 통해 FLIP의 예측과 어텐션 웨이트를 사용하여 T-FLIP의 예측 정확도를 높이며, 특히 작은 모델에서도 성능 저하가 발생하지 않도록 보완한다. <br> <br> 아래에서는 FLIP의 task loss 모듈과 지식 증류 모듈의 구현에 대해 더 자세히 기술한다. 먼저 T-FLIP이 직접 학습하는 과정은 다음 그림에서 나타난다. <br> <img width="700" alt="architecture" src="https://github.com/sudaltokki/T-FLIP/blob/main/img/task.png"> <br> T-FLIP의 직접 학습 부분에서는 Cross Entropy Loss (CE Loss), SimCLR Loss, 그리고 Mean squared error loss (MSE Loss)라는 세가지 손실 함수를 사용하여 모델을 최적화한다. 각 손실 함수는 서로 다른 학습 목표를 가지며, 각기 다른 관점에서 모델 성능을 향상시키는 역할을 한다. <br><br> 1.	Cross Entropy Loss <br>     A.	Cross Entropy Loss는 T-FLIP 모델이 예측한 확률 분포와 실제 레이블 간의 차이를 측정하는 손실함수이다. 이는 분류 작업에서 모델이 올바르게 학습되었는지를 평가하는 데 필수적이다. <br> B.	구현 측면에서, Cross Entropy Loss는 모델의 최종 출력 레이어에 적용되며, 실제 레이블을 one-hot encoding하여 모델의 예측 확률과 비교한다. 이 과정을 통해 모델은 잘못 예측된 클래스에 대해 더 큰 페널티를 받고, 이를 통해 점진적으로 예측 성능이 향상된다. <br><br> 2.	SimCLR Loss <br> A.	SimCLR Loss는 같은 이미지르 서로 다른 두가지 변형으로 생성하여 두 이미지를 모델에 입력한 후, 동일한 이미지로부터 추출된 특징이 일관되도록 학습하는 손실함수이다. <br> B.	구체적으로, T-FLIP모델은 동일한 입력 이미지에 다양한 변환 (크기 조절, 색상 변화 등)을 적용하여 두가지 서로 다른 버전을 만든다. 이 두 버전은 모델의 이미지 인코더를 통과하며, 인코더는 서로 다른 변형에서도 이미지의 본질적 특징을 일관되게 표현하도록 학습된다. 이를 통해 T-FLIP 모델은 시각적 일관성을 높이고, 다양한 데이터셋에서도 안정적인 성능을 보일 수 있다. <br><br> 3.	MSE Loss <br> A.	MSE Loss는 이미지-텍스트 쌍 간의 일관성을 유지하도록 모델을 학습시키기 위해 사용된다. 동일한 클래스에 속하는 두개의 무작위 텍스트 프롬프트와 이미지 특징 벡터 간의 평균 제곱 오차를 계산하여 일관된 표현을 추출하도록 학습시킨다. <br> B.	구현에서는, 동일 클래스에 속하는 이미지와 텍스트 쌍을 인코딩한 후, 두 표현 벡터 간의 차이를 MSE로 계산한다. 이 손실을 최소화함으로써 이미지와 텍스트 간의 높은 일관성을 유지하게 되며, 이는 특히 다양한 입력 소스가 있는 환경에서 모델의 일반화 성능을 개선하는 데 중요한 역할을 한다. <br> <br> 기존의 지식 증류 방법론인 특징 기반 지식 증류와 관계 기반 지식 증류 기법들을 사용했을 때, 제한적인 성능 향상을 보였기 때문에, T-FLIP은 새로운 접근법인 어텐션 가중치 기반의 지식 증류를 통해 교사 모델인 FLIP의 성능을 더욱 효과적으로 모방하도록 설계되었다. T-FLIP에서 사용한 지식 증류 모듈의 자세한 구조는 아래 그림과 같다. <br> <img width="700" alt="architecture" src="https://github.com/sudaltokki/T-FLIP/blob/main/img/att.png"> <br> 1.	Self-Attention layer MSE Loss 적용 <br> A.	T-FLIP의 이미지 인코더로 사용되는 ViT (Vision Transformer) 모델의 각 self-attention layer에 MSE 손실을 적용하여, 교사 모델의 attention map과 학생 모델의 attention map이 일치하도록 학습시킨다. <br> B.	구체적으로 FLIP 모델의 각 self-attention layer의 가중치와 T-FLIP의 동일한 레이어에서의 가중치를 비교하여 MSE loss를 계산한다. 이 손실 값을 최소화함으로써, 학생 모델의 attention이 교사 모델의 attention 패턴을 따라가도록 유도한다. <br><br> 2.	레이어별 동기화 과정 <br> A.	각 Self-attention layer는 서로 다른 수준의 정보를 다루기 때문에 레이어 별로 attention 패턴을 세밀하게 조정하는 작업이 필요하다. <br> B.	T-FLIP 모델은 각 레이어에서 개별적으로 attention alignment를 적용하여, 모든 레이어에서 일관된 패턴으로 교사 모델의 attention을 모방한다. 이 방식은 단순히 최종 출력이 일치하도록 하는 것이 아니라, 중간 단계의 특징을 세밀하게 조정하여 학생 모델이 교사 모델과 유사한 방식으로 정보를 처리하도록 돕는다. <br><br> |  
| (4) 주요 기능의 구현 |  |

