<!-- Template for PROJECT REPORT of CapstoneDesign 2024-2H, initially written by khyoo -->
<!-- 본 파일은 2024년도 컴공 졸업프로젝트의 <1차보고서> 작성을 위한 기본 양식입니다. -->
<!-- 아래에 "*"..."*" 표시는 italic체로 출력하기 위해서 사용한 것입니다. -->
<!-- "내용"에 해당하는 부분을 지우고, 여러분 과제의 내용을 작성해 주세요. -->

# Team-Info
| (1) 과제명 | *T-FLIP: Lightweighting CLIP for Face Anti-Spoofing via Knowledge Distillation*
|:---  |---  |
| (2) 팀 번호 / 팀 이름 | *04-티라노* |
| (3) 팀원 역할 분담 |  박지원(2076169) : 리더, *AI 모델 개발*   <br> 류이정(2176129) : 팀원, *AI 모델 개발*   <br> 소예림(2071028) : 팀원, *AI 모델 개발*  |
| (4) 팀 지도교수 | 심재형 교수님 |
| (5) 팀 멘토 | 최종원 / 조교수 / 중앙대학교 첨단영상대학원 영상학과 |
| (6) 과제 분류 | 연구 과제 |
| (7) 과제 키워드(keywords) | CLIP, Face Anti Spoofing(FAS), 경량화, knowledge distillation |
| (8) 과제 내용 요약 | FLIP(Face Anti-Spoofing with Language-Image Pretraining)은 멀티모달 모델인 CLIP(Contrastive Language-Image Pretraining)을 Face Anti-Spoofing(FAS) 태스크에 최적화한 모델이다. FLIP은 위조 얼굴을 탐지하는 FAS 태스크에서 다른 모델들에 비해 우수한 성능을 보여준다. 그러나 거대한 모델 구조로 인해 추론에 많은 비용이 발생하며, 엣지 디바이스에 탑재하는 데에 어려움이 있다. 이에 따라, Knowledge Distillation(지식 증류) 방법을 사용하여 FLIP의 우수한 성능을 유지하면서도 더 빠르게 추론할 수 있는 경량화 모델을 개발하고자 한다. |
<br>

# Project-Summary
| 항목 | 내용 |
|:---  |---  |
| (1) 문제 정의 | Face Anti-Spoofing(FAS)는 보안이 중요한 응용 프로그램에서 사용되는 얼굴 인식 시스템의 필수 요소이다. 현대 사회에서는 휴대폰 잠금 해제부터 공항 탑승 게이트까지 FAS 기술이 우리 생활에 밀접하게 사용되고 있다. FAS는 보안과 직결되는 기술이지만 더불어 매우 흔하고, 빈번하게 사용되는 기술이기 때문에 우리는 높은 성능 뿐 만 아니라 빠른 추론 속도의 필요성 또한 절감했다. 어디서든 보안이 필요한 곳이라면 컴퓨팅 능력과 자원이 한정된 디바이스에도 좋은 성능을 보일 수 있어야 한다. <br><br> 현재 연구에 사용되는 FLIP(Face Anti-Spoofing with Language-Image Pretraining) 모델은 CLIP(Contrastive Language-Image Pretraining) 기반의 멀티모달 모델로서, 이미지와 텍스트 간의 관계를 이해하는 데에 효과적이고 위조 얼굴 탐지(FAS) 태스크에서 뛰어난 성능을 보여준다. 그러나 169,568,513개의 파라미터를 가지고 있는 매우 크고 복잡한 모델 구조로 인해 추론 속도가 느리며 또 많은 비용이 발생한다. 이러한 이유로 FLIP 모델은 실시간 처리가 중요한 엣지 디바이스에 적합하지 않으며, 모바일 기기나 저사양 디바이스에서의 활용이 어렵다. <br><br> 따라서 Target Customer는 엣지 컴퓨팅 환경에서 보안이 중요한 애플리케이션을 사용하는 기업 및 개발자들이다. 이러한 고객들이 직면한 문제점은 다음과 같다:<br><br>- **엣지 디바이스 성능 제한**: 위조 얼굴 탐지와 같은 고성능 태스크는 엣지 디바이스에서 수행하기에 자원과 처리 속도 측면에서 많은 제약을 받는다.<br> - **실시간 위조 탐지 필요성**: 보안성 향상을 위해 위조 얼굴을 실시간으로 탐지하는 기능이 필수적이지만, FLIP 모델의 큰 구조로 인해 처리 속도가 느려 실시간 추론에 적합하지 않다.<br> - **추론 비용 문제**: 큰 모델을 클라우드 기반으로 운영할 경우 비용이 많이 들고, 이를 엣지 디바이스에서 수행할 수 있는 경량화된 대안이 필요하다.|
| (2) 기존연구와의 비교 | 본 과제는 FLIP 모델을 기반으로 FAS 작업에서의 성능과 효율성을 모두 향상시킨 모델 T-FLIP을 개발하는 것을 목표로 한다. FLIP을 경량화 하고자 하는 본 과제를 기존 연구들과 비교하여 FAS 과제에서 해당 FLIP 모델을 선택한 이유와, 이 모델을 경량화함으로써 갖는 장점을 설명해보고자 한다. <br><br> 1. 비전 트랜스포머(ViT) 모델을 사용한 FAS 연구[1][2] : <br> 최근 ViT 모델은 이미지 패치 간의 장거리 의존성을 포착하는 능력 덕분에 FAS 작업에 효과적인 것으로 나타났다. 그러나 해당 연구에는 두 가지 한계점이 있다. 첫째, 이미지 데이터만을 사용하여 학습하기 때문에 제한된 학습 데이터를 사용할 때 일반화 능력을 제한한다. 둘째, 사전 학습된 가중치를 미세 조정하기 위해 추가적인 네트워크 수정이나 도메인 레이블과 같은 추가 정보 구성 작업을 필요로 한다. 반면, T-FLIP에서 선택한 FLIP 모델은 이미지와 텍스트를 결합한 멀티모달 모델 사전학습 가중치로 ViT를 초기화하는 과정을 거쳐 더욱 향상된 FAS 성능을 보여준다. 또한, 이미지 표현을 클래스를 설명하는 텍스트 집합과 정렬하기 때문에, 데이터가 적은 상황에서도 FAS의 일반화 성능이 향상된다. <br><br> 2. CLIP(Contrastive Language-Image Pretraining) Knowledge distillation 연구[3][4][5] : <br> 국내외에서는 이미지와 텍스트를 동시에 이해하는 멀티모달 인공지능 모델의 개발이 활발히 진행되고 있다. FLIP 모델은 멀티모달 모델인 CLIP을 FAS 태스크에 최적화시킨 모델로서 CLIP의 아키텍쳐에 그 기반을 두고 있다. CLIP 모델을 경량화하고자 했던 이전 연구를 살펴보면, Tiny-CLIP, CLIP-KD, RKD 등의 연구는 CLIP과 유사한 성능을 유지하면서 파라미터 수를 효과적으로 감소시켜 다양한 knowledge distillation 기법의 효용성을 입증했다. 그러나 CLIP은 FAS 작업에 최적화된 모델이 아닐 뿐 더러, CLIP과 유사한 구조를 가진 FLIP에 대해서는 아직 경량화 시도가 진행된 바가 없다. 따라서 본 프로젝트에서는 FAS 과제에 최적화된 모델의 경량화를 해내고자 FLIP에 knowledge distillation을 적용하였다. <br><br> 위와 같은 이유로 본 연구는 기존에 시도되지 않았던 FLIP 모델을 knowledge distillation으로 경량화하는 독창적인 시도를 통해 T-FLIP이 비교적 우수한 FAS 성능과 빠른 추론 속도를 가질 것이라 예상하며, 나아가 실제 환경에서 FAS 태스크의 적용 가능성을 높일 것이라 기대한다. <br><br> 출처 : <br>  [1]  Anjith George and Se ́bastien Marcel. On the effectiveness of vision transformers for zero-shot face anti-spoofing. In 2021 IEEE International Joint Conference on Biometrics (IJCB), pages 1–8. IEEE, 2021. <br> [2] Hsin-PingHuang,DeqingSun,YaojieLiu,Wen-ShengChu, Taihong Xiao, Jinwei Yuan, Hartwig Adam, and Ming- Hsuan Yang. Adaptive transformers for robust few-shot cross-domain face anti-spoofing. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XIII, pages 37–54. Springer, 2022. <br> [3] Wu, Kan, et al. "Tinyclip: Clip distillation via affinity mimicking and weight inheritance." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023. <br> [4] Yang, Chuanguang, et al. "CLIP-KD: An Empirical Study of Distilling CLIP Models." arXiv preprint arXiv:2307.12732, 2023. <br> [5] Park, Wonpyo, et al. "Relational knowledge distillation." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.|
| (3) 제안 내용 | 1. FLIP 모델 경량화의 첫번째 시도 : 딥러닝 모델의 경량화 연구는 다양한 분야에서 활발히 진행되고 있지만, 최근 얼굴 위조 탐지(Face Anti-spoofing, FAS) 분야에서 뛰어난 성능을 보인 FLIP(FLIP: Cross-domain Face Anti-spoofing with Language Guidance) 모델에 대한 경량화 연구는 아직 이루어지지 않았다는 점이 주목할 만하다. FLIP 모델은 그 우수한 성능에도 불구하고, 약 1억개 이상의 대규모 파라미터와 느린 추론 속도로 인해 실제 응용에 제약이 있다. 본 연구 프로젝트는 knowledge distillation(KD)를 도입하여 FAS의 추론 속도를 향상시키고 FLIP 모델의 한계를 극복하고자 한다. <br><br> 2. 다양한 KD 방법론의 비교 : 본 연구는 FLIP 모델의 고유한 특성을 최대한 활용하기 위해 다양한 KD 기법을 적용하며 최종적으로 FLIP을 가장 잘 경량화하는 KD 방법론을 찾아가는 것에 의의가 있다. 특히, 단순한 logit 기반의 distillation을 넘어서 feature distillation과 relational knowledge distillation(RKD)의 장점을 결합한 접근 방식을 채택할 예정이다. 이를 통해 목표 성능이었던 90%이상의 AUC, 2배 이사의 추론속도 향상을 달성해냈으며, 추가적인 성능 향상을 위해 새로운 방법론을 탐색 중이다. <br><br> 3. FLIP에 적합한 KD 방식 채택 : 다양한 KD 방법론을 적용해본 경험에 따르면, 어텐션 맵에 중점을 둔 distillation이 FLIP 모델의 성능을 경량 모델에 효과적으로 전달할 수 있을 것이라 기대한다. 이는 학생모델로 하여금 교사모델이 특정한 시각적 특징에 집중하는 방식을 학습하는 데 도움이 될 것이고 최종적으로 T-FLIP의 성능 향상을 이뤄낼 수 있을 것이다. <br><br> 이러한 다면적 접근은 FLIP 모델의 핵심 강점인 언어 가이던스와 크로스 도메인 능력을 경량 모델에 더욱 효과적으로 전달할 수 있을 것으로 기대된다.<br> 즉, 본 연구는 계산 효율성을 크게 향상시키면서도 원래 FLIP 모델에 근접한 성능을 가진 경량화된 모델 T-FLIP을 개발하는 것을 목표로 한다.|
| (4) 기대효과 및 의의 | 본 연구 모델 T-FLIP의 목표는 FAS 기술을 사용하고자 하는 모든 기술 및 기업을 목표 사용자로 하여, FLIP (Face Anti-Spoofing with Language-Image Pretraining) 모델의 추론 속도를 향상시켜 보다 효율적이고 실용적인 사용 환경을 조성하는 것이다. <br><br> 구체적으로는 다음과 같은 목표를 설정하였다:<br> 1. FLIP의 추론 속도를 기존보다 최소 2배 이상 향상시키는 것을 목표로 한다. 이를 통해 FLIP 모델의 사용성을 향상시키고, 실제 환경에서의 빠른 응답이 가능하도록 한다.<br> 2. T-FLIP 모델에게도 FLIP과 유사한 성능을 제공하여 모델의 크기를 줄이고 자원을 효율적으로 활용할 수 있도록 한다. 이를 통해 모바일 및 임베디드 시스템에서도 FLIP 기술을 적용할 수 있는 환경을 조성한다.<br> 3. 추론 속도의 향상이 모델의 정확도에 부정적인 영향을 미치지 않도록 한다. 즉, 정확도는 90% 이상을 유지할 수 있도록 하며 속도와 정확도 간의 균형을 유지하여 실제 사용 시에도 뛰어난 성능을 제공한다.<br> 4. FLIP 모델의 개선을 통해 기업들은 보다 빠르고 정확한 FAS 인공지능 서비스를 제공할 수 있게 되어 산업 혁신과 경쟁력 강화에 기여할 것으로 기대된다.<br><br> T-FLIP을 통해 위 목표를 달성함으로써, FAS 태스크의 보안성을 높여 기술 사용자들의 불안함을 해소하고, FLIP 기반의 다중 모달 인공지능 모델을 보다 효율적으로 개발하고 실제 응용 환경에서의 적용 가능성을 높이고자 한다.|
| (5) 주요 기능 리스트 | 1. FAS task : T-FLIP은 FAS task 기능을 수행한다. 특정 사진 입력에 대해서 해당 사진이 위/변조 이미지인지의 여부를 판별하고 그 결과를 출력한다. 위/변조 이미지에는 다양한 카메라 렌즈를 통해 촬영된 얼굴 사진(종이 또는 천), 얼굴 촬영 비디오 등이 포함된다. 위/변조 이미지 데이터는 FAS task에 자주 사용되는 MSU-MFSD, CASIA-MFSD, Replay-Attack, OULU-NPU의 4개 데이터를 사용했다. <br><br> 2. 지식 증류를 활용한 모델 경량화 : T-FLIP은 FAS task를 수행하는 거대 모델 FLIP을 경량화하여 보다 적은 자원으로 유사한 성능을 제공하도록 개발된 모델이다. 지식 증류에는 기존의 다양한 지식 증류 기법(FD, RKD)과 직접 고안한 Attention map distillation을 적용해보았다. Attention map distillation을 사용하여 가장 높은 성능을 보였으며 T-FLIP에는 해당 지식 증류 기법을 적용한다. <br><br> (1) FD(Feature Distillation): <br> knowledge distillation 방법 중 교사 모델(teacher model)에서 추출한 피처(feature)를 학생 모델(student model)로 증류하는 방법이다. 교사 모델은 학습 과정에서 여러 단계의 중간 피처를 생성하는데, 이 피처들은 입력 데이터의 특정 속성을 표현하는 중요한 정보이다. FD의 핵심은 교사 모델의 여러 레이어에서 추출된 피처맵을 학생 모델로 전이시켜, 교사 모델이 학습한 중요한 정보(피처)를 학생 모델이 학습하도록 돕는 것이다. 이는 단순히 출력 logits를 맞추는 방식보다 더 세밀하게 정보를 전달하는 방법으로, 학생 모델이 더 효율적으로 학습할 수 있도록 한다. <br><br> (2) RKD(Relational Knowledge Distillation): <br> Relational Knowledge Distillation(RKD)은 데이터 간의 관계성(relations)을 기반으로 지식을 증류하는 방법이다. 일반적인 피처 증류는 교사 모델과 학생 모델 간의 개별 데이터에 대한 매핑에 초점을 맞추는 반면, RKD는 여러 데이터 간의 상호작용이나 관계성을 학습하고 그 관계성을 학생 모델로 전이시키는 것을 목표로 한다. 이 방법론은 교사 모델이 학습한 데이터 간의 관계성(데이터 포인트 간의 거리, 상대적 위치, 유사성)을 학생 모델이 학습함으로써, 데이터 간의 구조적 정보를 효과적으로 전달한다. 특히 거리(distances)와 각도(angles)를 사용하여 데이터 간의 상대적 관계를 학습한다. 이를 통해 학생 모델이 데이터의 구조적 특성을 보존할 수 있도록 유도한다. <br><br> (3) Attention map distillation: <br> Attention map distillation은 레이어에서 학생 모델과 교사 모델이 주목하는 부분을 일치시키고, 결과적으로 교사 모델이 각 레이어에서 학습한 중요한 특징들을 학생 모델이 효과적으로 모방할 수 있도록 유도한다. 이는 고차원 특징과 위/변조 단서 탐지 등 레이어 별로 집중해야 할 부분을 학생 모델이 더 잘 학습하도록 돕는다.|

<br>
 
# Project-Design & Implementation
| 항목 | 내용 |
|:---  |---  |
| (1) 요구사항 정의 | <img width="787" alt="스크린샷 2024-11-07 오후 4 16 54" src="https://github.com/user-attachments/assets/b80d8dce-7ff5-460e-965d-27912359c79b"> <br> 우선 데이터셋 관련한 요구사항을 설명하자면, 모델 훈련과 모델 성능 검증을 위한 데이터셋이 필요하다. 데이터셋은 잘 알려져있는 FAS 데이터셋에 해당하는 MSU-MFSD, CASIA-MFSD, Replay-Attack, OULU-NPU로 결정했다. 모델을 훈련시키고 성능을 평가할 때는 이 중 임의로 세개를 선택하여 학습 데이터셋으로 사용하고 나머지 한 개를 테스트 데이터셋으로 사용한다. <br> T-FLIP의 모델 요구사항에는 처리시간 및 파라미터 수 감소, 목표 정확도, 장비 사양 등이 있다. 우선 추론을 진행하는 데에 소요되는 처리시간과 파라미터 수를 감소시키는 것이 프로젝트의 목표이므로, 처리 시간을 두배 빠르게 만든다는 관련 요구사항에 따라 모델을 개발하고 있다. 다음으로 목표 정확도 관련 요구사항은 90% 이상을 유지하는 것이다. Knowledge distillation을 적용하되 작은 모델에서도 좋은 성능을 낼 수 있도록 최소 90% 이상의 정확도를 가질 수 있도록 모델을 개발한다. 추가적으로 모델을 훈련 시킬 때 필요한 장비인 GPU 서버는 NVIDIA GeForce RTX 4090로, 프로젝트 지도교수님인 심재형 교수님의 지원을 받아 사용하고 있다. |
| (2) 전체 시스템 구성 | <img width="740" alt="스크린샷 2024-11-07 오후 4 17 02" src="https://github.com/user-attachments/assets/6f1011c8-d68b-4d4e-bb51-a212dfdac50a"> <br> 위 그림은 T-FLIP의 전체 시스템 구조이다. FLIP 모델을 교사모델로 삼아 경량화하고자 하였으며, FLIP git( https://github.com/koushiksrivats/FLIP.git )을 통해 코드를 받아 경량화 코드 개발을 진행했다. 데이터로는 FAS 데이터로 자주 사용되는 MSU-MFSD, CASIA-MFSD, Replay-Attack, OULU-NPU의 4개 데이터를 사용한다. MSU-MFSD는 실제 얼굴과 위변조 얼굴에 대한 35명의 비디오를 녹화한 데이터로 총 280개의 비디오를 가지고 있다. CASIA-MFSD는 인당 12개씩, 50명의 비디오를 서로 다른 해상도와 조명 조건 아래에서 녹화한 데이터셋이다. Replay-Attack는 50명의 비디오, 총 1300개의 비디오로 구성되어 있으며 실제 얼굴 외에 얼굴 사진 및 비디오를 녹화한 영상들로 이루어져 있다. 마지막으로 OULU-NPU는 조명조건과 배경장면이 다른 4950개의 실제 얼굴 및 위변조 얼굴 비디오로 구성되어 있다. 위 데이터셋 중 훈련 단계에서는 3개의 데이터셋만 활용하고, 테스트 단계에서는 학습에 사용하지 않은 1개의 데이터셋으로 평가하여 새로운 도메인에서도 효과적으로 대응할 수 있는지를 검증한다. 이를 통해 T-FLIP의 일반화 능력을 평가한다. <br><br> T-FLIP의 핵심 모듈은 모델이 직접 학습하는 task loss 모듈과, 지식 증류 모듈이다. 각 모듈은 다음과 같은 역할을 수행한다.<br> <img width="932" alt="스크린샷 2024-11-07 오후 4 27 46" src="https://github.com/user-attachments/assets/43a1c44f-409d-4f52-9bf9-462997d533dd"> <br> 1.	FLIP의 task loss 모듈: 위의 그림은 경량화 모델을 훈련시키는 데에 쓰이는 모듈이다. T-FLIP은 3개의 데이터셋을 사용하여 직접 얼굴 위조 탐지에 필요한 특징을 학습한다. 이 과정에서는 여러 데이터셋으로부터 다양한 도메인 특징을 학습하여 모델이 넓은 범위의 도메인에 대해 강인한 일반화 성능을 가지도록 한다. 먼저 simCLR Loss는 동일한 이미지에서 두개의 뷰를 생성하고, 두 변환된 이미지를 이미지 인코더와 비선형 투영 네트워크에 투영하여 얻은 특징에 대조학습을 적용한다. 이 과정을 통해 모델은 이미지 변환에 관계없이 일관된 특징을 학습하게 된다. <br> CE Loss는 Image Encoder와 Text Encoder의 대조학습의 결과로 나타난 클래스의 확률 분포가 실제 정답과 어느정도 차이가 나는지 계산할 때 사용된다. 마지막으로 MSE loss는 이미지-텍스트 간의 유사성 점수를 계산할 때 사용된다. 동일한 이미지-텍스트 쌍이 일관된 유사성 점수를 가지도록 강제하게 된다. <br><br> <img width="430" alt="attention" src="https://github.com/user-attachments/assets/ab340b79-6635-4158-8a6f-98dbc0c62320"> <br> 2.	지식 증류 모듈: 지식 증류는 T-FLIP에서 가장 핵심적인 역할을 한다. 이 모듈은 교사 모델인 FLIP 모델로부터 중요한 지식을 T-FLIP에 전달하는 역할을 한다. Teacher 모델이 데이터에 대해 주목하는 부분을 학생 모델에게 전달하여 학생 모델이 중요한 피처에 집중할 수 있도록 학습을 진행한다. 지식 증류 기법을 통해 FLIP의 예측과 어텐션 웨이트를 사용하여 T-FLIP의 예측 정확도를 높이며, 특히 작은 모델에서도 성능 저하가 발생하지 않도록 보완한다. <br>|  
| (3) 주요엔진 및 기능설계 | 아래에서는 FLIP의 task loss 모듈과 지식 증류 모듈의 구현에 대해 더 자세히 기술한다. 먼저 T-FLIP이 직접 학습하는 과정은 다음 그림에서 나타난다. <br> <img width="932" alt="스크린샷 2024-11-07 오후 4 27 46" src="https://github.com/user-attachments/assets/a37843ab-06c9-4e48-96e2-8bde7115c7bc"> <br> T-FLIP의 직접 학습 부분에서는 Cross Entropy Loss (CE Loss), SimCLR Loss, 그리고 Mean squared error loss (MSE Loss)라는 세가지 손실 함수를 사용하여 모델을 최적화한다. 각 손실 함수는 서로 다른 학습 목표를 가지며, 각기 다른 관점에서 모델 성능을 향상시키는 역할을 한다. <br><br> 1.	Cross Entropy Loss <br>     A.	Cross Entropy Loss는 T-FLIP 모델이 예측한 확률 분포와 실제 레이블 간의 차이를 측정하는 손실함수이다. 이는 분류 작업에서 모델이 올바르게 학습되었는지를 평가하는 데 필수적이다. <br> B.	구현 측면에서, Cross Entropy Loss는 모델의 최종 출력 레이어에 적용되며, 실제 레이블을 one-hot encoding하여 모델의 예측 확률과 비교한다. 이 과정을 통해 모델은 잘못 예측된 클래스에 대해 더 큰 페널티를 받고, 이를 통해 점진적으로 예측 성능이 향상된다. <br><br> 2.	SimCLR Loss <br> A.	SimCLR Loss는 같은 이미지르 서로 다른 두가지 변형으로 생성하여 두 이미지를 모델에 입력한 후, 동일한 이미지로부터 추출된 특징이 일관되도록 학습하는 손실함수이다. <br> B.	구체적으로, T-FLIP모델은 동일한 입력 이미지에 다양한 변환 (크기 조절, 색상 변화 등)을 적용하여 두가지 서로 다른 버전을 만든다. 이 두 버전은 모델의 이미지 인코더를 통과하며, 인코더는 서로 다른 변형에서도 이미지의 본질적 특징을 일관되게 표현하도록 학습된다. 이를 통해 T-FLIP 모델은 시각적 일관성을 높이고, 다양한 데이터셋에서도 안정적인 성능을 보일 수 있다. <br><br> 3.	MSE Loss <br> A.	MSE Loss는 이미지-텍스트 쌍 간의 일관성을 유지하도록 모델을 학습시키기 위해 사용된다. 동일한 클래스에 속하는 두개의 무작위 텍스트 프롬프트와 이미지 특징 벡터 간의 평균 제곱 오차를 계산하여 일관된 표현을 추출하도록 학습시킨다. <br> B.	구현에서는, 동일 클래스에 속하는 이미지와 텍스트 쌍을 인코딩한 후, 두 표현 벡터 간의 차이를 MSE로 계산한다. 이 손실을 최소화함으로써 이미지와 텍스트 간의 높은 일관성을 유지하게 되며, 이는 특히 다양한 입력 소스가 있는 환경에서 모델의 일반화 성능을 개선하는 데 중요한 역할을 한다. <br> <br> 기존의 지식 증류 방법론인 특징 기반 지식 증류와 관계 기반 지식 증류 기법들을 사용했을 때, 제한적인 성능 향상을 보였기 때문에, T-FLIP은 새로운 접근법인 어텐션 가중치 기반의 지식 증류를 통해 교사 모델인 FLIP의 성능을 더욱 효과적으로 모방하도록 설계되었다. T-FLIP에서 사용한 지식 증류 모듈의 자세한 구조는 아래 그림과 같다. <br> <img width="430" alt="attention" src="https://github.com/user-attachments/assets/52df718d-1ad7-4a9d-893a-7652eee89d2e"> <br> 1.	Self-Attention layer MSE Loss 적용 <br> A.	T-FLIP의 이미지 인코더로 사용되는 ViT (Vision Transformer) 모델의 각 self-attention layer에 MSE 손실을 적용하여, 교사 모델의 attention map과 학생 모델의 attention map이 일치하도록 학습시킨다. <br> B.	구체적으로 FLIP 모델의 각 self-attention layer의 가중치와 T-FLIP의 동일한 레이어에서의 가중치를 비교하여 MSE loss를 계산한다. 이 손실 값을 최소화함으로써, 학생 모델의 attention이 교사 모델의 attention 패턴을 따라가도록 유도한다. <br><br> 2.	레이어별 동기화 과정 <br> A.	각 Self-attention layer는 서로 다른 수준의 정보를 다루기 때문에 레이어 별로 attention 패턴을 세밀하게 조정하는 작업이 필요하다. <br> B.	T-FLIP 모델은 각 레이어에서 개별적으로 attention alignment를 적용하여, 모든 레이어에서 일관된 패턴으로 교사 모델의 attention을 모방한다. 이 방식은 단순히 최종 출력이 일치하도록 하는 것이 아니라, 중간 단계의 특징을 세밀하게 조정하여 학생 모델이 교사 모델과 유사한 방식으로 정보를 처리하도록 돕는다. <br><br> <실험 내용> <br><br> 위와 같은 모듈을 활용하여 성능 확인을 위해 실험을 진행했다. 구체적인 실험 내용은 다음과 같다. <br> 1. 입력 이미지 데이터를 일정 비율로 크롭을 적용하고, 크기를 224×224×3로 변환한 후 16×16 크기의 패치로 분할한다. <br> 2. 이미지 인코더로는 ViT-tiny를 사용했으며 텍스트 입력은 FLIP에서 사용한 텍스트 프롬프트를 활용하였다. <br> 3. 모델 학습 시 초기 학습률은 10^(-3), 배치 사이즈는 8로 설정했고, 가중치 감쇠는 10^(-1)로 설정하여 과적합을 방지했다. <br>  4. Adam Optimizer을 사용해 모델 학습을 진행했다. <br> 5. 교차 도메인 성능 평가를 위해서 M,C,I,O 네 개의 데이터셋 중 세개를 사용하여 학습한 후 나머지 하나를 사용해 추론을 진행하여 총 네가지의 시나리오에서 성능을 검증했다. <br><br> 진행한 실험 목록은 다음과 같다. <br> 1. FLIP 모델 중 가장 일반화 성능이 뛰어난 FLIP-MCL의 성능 평가 <br> 지식 증류를 적용하지 않고 task loss로만 학습을 진행한 학생 모델 성능 평가 <br> 3. FD를 사용하여 지식 증류한 학생 모델 성능 평가 <br> 4. RKD를 사용하여 지식 증류한 학생 모델 성능 평가 <br> 5. Attention map distillation을 사용하여 지식 증류한 학생 모델 성능 평가 (T-FLIP) <br> 6. 학습 및 지식 증류의 성능 확인을 위해 Attention map 시각화 <br> 7. 교사 모델 대비 학생 보델 경량화 지표 확인 <br><br> 각 실험에서 사용한 모델의 평가 지표는 HTER(Half total error rate), AUC, FPR%TPR=1%이다. HTER은 False Positive Rate(FPR)와 False Negative Rate(FNR)의 평균으로, 모델의 전반적인 오류율을 나타낸다. AUC는 ROC(Receiver Operating Characteristic) 곡선 아래 영역의 넓이를 의미하며, 모델의 전반적인 성능을 평가한다.  |  
| (4) 주요 기능의 구현 | T-FLIP의 주요 기능은 FAS task 수행과 지식 증류이다. 각 기능에 대한 실험 결과 및 성능을 구체적으로 기술하면 아래와 같다. <br><br> 아래의 표는 기존의 경량화되지 않은 FAS 모델들, 교사 모델이 FLIP, 그리고 다양한 지식 증류 기법을 적용한 결과와 비교하여 T-FLIP의 교차 도메인 성능 평가를 제시한다.<br>  <br> 1. FAS task 수행 능력 : <img width="1193" alt="스크린샷 2024-12-12 오후 5 22 01" src="https://github.com/user-attachments/assets/255bcc5d-8bf1-4e94-b0ec-31257636a957" /> <br> FAS task 수행 능력은 모델의 성능으로 확인할 수 있다. T-FLIP의 학습에 활용된 기존의 FLIP-MCL의 성능은 교차 도메인 성능 평가 네 가지 시나리오 모두에서 AUC 지표가 98 이상으로 매우 뛰어난 성능을 보인다. T-FLIP 또한 기존 FAS 모델들과 비교했을 때 대등하거나 더 뛰어난 성능을 보여준다. 이는 다양한 시나리오에서 입력에 대한 위변조 유무를 제대로 판별할 수 있다는 것을 의미하며 뛰어난 일반화 능력을 보여준다. <br><br> 2. 지식 증류 : <br> (1) T-FLIP은 FLIP 모델에 적합한 지식 증류 기법을 활용함으로써 효과적인 경량화와 동시에 높은 성능을 제공할 수 있도록 한다. 표에서 지식 증류를 적용하지 않은 학생 모델(학습된 학생 모델), 다른 지식 증류 기법인 FD(특징 기반 지식 증류), RKD(관계 기반 지식 증류), Attention map distillation(T-FLIP)의 성능을 확인할 수 있다. T-FLIP은 지식 증류를 적용하지 않은 학생 모델에 비해 뛰어난 성능을 보이며, FD나 RKD의 다른 지식 증류를 적용했을 때보다 더 우수한 성능을 가진다. 특히 OCI->M 시나리오에서는 더 우수한 결과를 보여준다. 또한 교사 모델인 FLIP과 비교했을 때 AUC가 약 2%만 하락한 경향을 보인다. 따라서 T-FLIP에서 제시한 Attention map distillation은 기존의 다른 지식 증류 기법에 비해 뛰어난 성능을 보인다. <br><br> (2) 다음으로 지식 증류를 활용한 경량화로 인해 모델의 추론 속도와 크기가 얼마나 감소했는지 확인해보았다. <br> <img width="937" alt="스크린샷 2024-12-12 오후 4 34 37" src="https://github.com/user-attachments/assets/2b47c0b8-9b1f-488d-b26c-736fad65e5c6" /> <br> 위 표는 교사 모델과 학생 모델 간의 구조와 파라미터 개수 및 연산량의 차이를 보여준다. parameter수는 모델에서 조정될 수 있는 변수의 개수를, FLOPs는 모델의 계산 복잡성으로 연산량을 나타낸다. Throughput은 일정시간동안 처리할 수 있는 데이터의 양으로 처리 속도를 나타낸다. 학생 모델인 T-FLIP은 교사 모델인 FLIP-MCL에 비해 30.8%의 파라미터 개수와 8.6%의 연산량을 가진다. 이로 인해 T-FLIP의 추론 속도는 FLIP-MCL 대비 2.5배 향상되었다. <br><br> 따라서 T-FLIP은 FAS task 수행과 효과적인 지식 증류 두 가지의 주요 기능을 올바르게 제공하고 있음을 확인할 수 있다.|

<br>
 
# Evaluation
| 항목 | 내용 |
|:---  |---  |
| (1) 평가(Evaluation) | 1. **평가항목 선정** <br>: 구체적인 평가지표를 구분하여 T-FLIP 모델을 평가해보고자 한다. 우선, FAS 모델 경량화라는 프로젝트의 목적에 맞게 T-FLIP이 교사모델에 비하여 얼마나 경량화되었는지를 평가하였다. 다양한 경량화 지표를 선정하고 교사모델과의 비교를 통해서 우리 프로젝트의 주요 목적을 달성하였는지 살펴보았다. 다음으로, 성능을 평가해보고자 한다. T-FLIP은 모델의 크기를 줄이는 것 뿐 만 아니라 교사모델 FLIP의 우수한 성능을 최대한 유지할 수 있는 효과적인 경량화를 목표로 하였다. 다른 모델들과의 성능 비교를 통해서 우리가 효율적인, 좋은 성능의 모델을 개발하였는지 확인해보았다. <br><br><br> 2. **평가항목 A : 모델 경량화 평가** <br> (1) 경량화란 모델 크기 최적화, 즉 대규모 및 복잡한 인공지능 모델을 작고 가벼운 형태로 변환하는 기술로 모델의 무게를 줄이거나 복잡도를 낮추는 것을 의미한다. 경량화의 목표는 주로 모델을 많은 GPU가 필요 없도록, 더 효율적으로 실행하고 자원이 제한된 환경에서도 모델의 적용 및 사용이 가능하게 만드는 것이다. 경량화 기법에는 Quantization, Pruning, knowledge Distillation 등의 방법이 있다. 우리는 커다란 모델의 지식을 작은 모델로 전달하여 작은 모델의 가벼움과 큰 모델의 성능을 모두 유지시킬 수 있는 Knowledge Distillation(지식증류) 방법을 사용하여 경량화를 진행하였다. 해당 경량화 평가에서는, 지식 증류를 사용하여 교사모델(FLIP-MCL)을 얼마나 경량화를 해내었는지 살펴보겠다. <br><br> (2) 판단 기준 <br>: 경량화 정도를 평가하기 위해서 우리는 모델의 계산 복잡성, 연산량을 의미하는 FLOPs, 모델에서 조정될 수 있는 변수의 개수인 parameter 수, 그리고 일정 시간 동안 처리할 수 있는 데이터의 양에 해당하는 throughput 세가지의 평가지표를 사용하였다. 경량화 정도는 프로젝트의 목표였던 FLOPs와 parameter 수는 50% 이하로 감소, 그리고 throughput은 두배 이상 향상이 되었을 때 경량화가 의미있게 되었다고 판단하였다. 추가적인 판단 기준은 각각의 평가지표에 따라 그 정도가 향상될 수록 성공적인 경량화로 분류하였다. 상세한 판단 기준은 아래와 같다. <br><br> (parameter 수, FLOPs, throughput) <br> - (50-70%, 50-70%,2-2.25배) : 보통, 경량화에서 흔히 달성 가능한 범위에 해당한다. <br> - (70-90%, 70-90%, 2.25-2.5배) : 다소 성공적 <br> - (90% 이상, 90% 이상, 2.5배 이상) : 매우 성공적, 이는 기존 모델의 구조적 한계를 극복한 매우 성공적인 경량화를 의미한다. 기존 모델의 근본적인 설계 변경이나 완전히 새로운 접근법을 도입했음을 시사한다. <br><br> (3) 평가 방식 <br> FLOPs : fvcore 패키지의 method인 FlopCountAnalysis를 사용하여 측정한다. FlopCountAnalysis는 분석할 pytorch 모델 인스턴스를 입력으로 받는다. 주어진 입력을 바탕으로 모델의 모든 연산을 추적하여 총 FLOPs를 계산한다. 결과는 dictionary 형태로 반환되며, 각 연산의 Flops와 전체 Flops의 합계를 포함한다. <br> Parameter 수 : fvcore의 parameter_count method를 사용하여 측정한다. 분석할 pytorch 모델 인스턴스를 입력으로 받아 모델에 포함된 모든 weight와 bias 파라미터의 개수를 집계하여 반환한다. <br> Throughput은 실제 실행 시간을 측정해야 하므로 timeit 모듈을 사용해 시간을 잰다. 모델의 forward method를 여러번 실행하여 평균 실행 시간을 측정한다. 단위 시간당 처리할 수 있는 입력 샘플의 개수를 throughput으로 계산한다.  <br><br><br> 3. **평가항목 B : 모델 성능 평가** <br> (1) T-FLIP의 목적은 FLIP의 높은 성능을 유지하며 크기를 줄이는 것이었으므로 성능 또한 높게 유지가 되었는지 평가해보았다. T-FLIP에서 성능이란 위변조 얼굴 자극을 실제 얼굴 자극과 얼마나 잘 구분해내는 지를 의미한다. 우리는 Attention weight-based KD라는 새로운 경량화 기법을 제안하였고, 이 지식 증류 방식이 얼마나 효과적으로 FLIP을 경량화 하였는지를 이 지표를 통해 평가할 수 있다. 4가지의 데이터셋 M, C, I, O 중 세가지의 데이터셋을 선택하여 training에 사용하고, 나머지 하나의 데이터셋을 사용해 inference를 진행했기 때문에 총 네가지의 평가 시나리오가 존재한다. 각 평가 시나리오에서 T-FLIP의 안면 위조 탐지 성능을 평가하였다. <br><br> (2) 판단 기준 <br>: 모델의 성능을 평가하기 위해서 HTER(Half total error rate), AUC, FPR%TPR=1%을 사용했다. HTER은 False Positive Rate(FPR)와 False Negative Rate(FNR)의 평균으로, 모델의 전반적인 오류율을 나타내기 때문에 낮을수록 좋다. AUC는 ROC(Receiver Operating Characteristic) 곡선 아래 영역의 넓이를 의미하며, 모델의 전반적인 성능을 평가하는 것으로 높을수록 성능이 좋음을 의미한다. <br> 판단 기준은 우선, **교사모델인 FLIP**에 비해서 성능이 얼마나 감소했는지를 비교해 보아 성능의 감소를 성공적으로 막아내었는지 판단하고자 했다. <br> 또한, **다른 FAS 모델**과 각각의 성능 지표들을 비교하여 FAS 모델 중에서도 T-FLIP이 경쟁력이 있는지 확인해보고자 했고, <br> **FLIP에 다른 지식 증류 방식을 적용한 경량화 모델**들과 T-FLIP을 비교하여 우리가 제안한 Attention weight-based KD가 FLIP을 다른 방식들에 비해 얼마나 효과적으로 경량화해냈는지 평가하고자 했다. 이 때 다른 지식 증류는 특징 기반 지식 증류와 관계 기반 지식 증류를 사용했다. 이유는 FLIP은 CLIP이라는 멀티모달 모델의 사전학습 가중치를 사용하는데, 특징 기반 지식 증류와 관계 기반 지식 증류가 CLIP의 경량화에서 효과적인 기법으로 알려져 있기 때문이다. 상세한 판단 기준은 아래와 같다. <br><br> (HTER, AUC) <br> - (1% 미만 차이남, 1% 미만 차이남) : 차이 없음, 차이가 작아 데이터의 노이즈나 무작위 요인 때문일 가능성이 크다. <br> - (1-3%, 1-3%) : 조금 높음, HTER이 낮은 영역에서는 중요한 개선으로 간주될 수 있으나 데이터 크기와 분포에 따라 의미 있는 개선 여부가 달라질 수 있다. 또한 AUC는 민감도와 특이도를 동시에 고려했을 때, 성능이 약간 향상되었음을 나타낸다.<br> - (3-6%, 3-6%) : 높음, HTER이 낮을수록 3% 이상의 감소는 모델 성능이 눈에 띄게 개선되었음을 나타내며 이는 실제 애플리케이션에서도 차이를 체감할 가능성이 높다. AUC 측면에서는 일반적으로 차이가 이 정도 나면, 모델 성능 개선으로 간주됨. <br> - (6-9%, 6-9%) : 많이 높음, HTER 기준 이 정도의 감소는 모델이 오탐과 미탐을 훨씬 더 잘 균형 있게 줄였음을 의미한다. AUC에서는 기존 모델 대비 큰 차이로 주목할 만하며 특히, 클래스 간의 차이가 애매했던 데이터를 더 잘 구분할 수 있게 된다. (9% 이상, 9% 이상) : 매우 높음, HTER의 감소 폭이 9% 이상이라면, 기존 모델의 한계를 뛰어넘는 성능이라는 의미이다. AUC에서의 9% 이상 향상은 모델이 거의 모든 데이터를 정확히 분류할 수 있다는 의미로 볼 수 있다. <br><br> (3) 평가 방식 <br> HTER : false acceptance rate (FAR)와 false rejection rate (FRR)의 평균으로 측정한다. 모델의 예측값과 라벨을 비교하여 true negative (TN), false negative (FN), false positive (FP), true positive (TP)를 측정한다. FAR=FP/(FP+TN), FRR=FN/(FN+TP)를 통해 FAR값과 FRR 값을 구하고, 두 값의 평균을 내어 HTER 값을 측정한다. <br> AUC : ROC 곡선 아래의 면적으로 sklearn의 roc_auc_score 함수를 사용하여 측정한다. roc_auc_score는 ground truth와 prediction을 인자로 받아, 이 관계를 바탕으로 ROC 곡선을 생성하고, 그 아래의 면적을 계산한다. <br><br><br> 4. **평가 내용 및 결과** <br><br> (1) 경량화 지표 비교 결과 <img width="937" alt="스크린샷 2024-12-12 오후 4 34 37" src="https://github.com/user-attachments/assets/1ef070aa-6a0b-47aa-b56b-0c7c6944c827" /> <br> Parameter 수는 149M에서 46M로 69.2퍼센트, 약 70퍼센트 감소하였고 FLOPs는 580.3G에서 50.2G로 91.4% 감소하였다. Throughput은 약 2.52배 증가하였다. 위의 평가 기준에 따라 분석해본 결과, parameter 수는 다소 성공적인 경량화에 가깝고, FLOPs는 매우 성공적인 경량화 달성에 해당했다. 또한 throughput도 매우 성공적으로 높아졌기 때문에 T-FLIP은 거대 FAS 모델인 FLIP 모델을 효과적으로 경량화한 성과를 보여준다. <br><br> (2) 성능 지표 비교 결과 <img width="872" alt="스크린샷 2024-12-12 오후 7 25 05" src="https://github.com/user-attachments/assets/3e66a626-470e-46f6-acd0-4029e5da00d6" /> <br> 우선 교사모델인 FLIP과 비교했을 때 HTER은 평균적으로 11% 증가했으며, AUC는 7% 감소하였다. 위 평가기준에 따르면 매우 차이가 많이 나는 것으로 FLIP의 우수한 성능을 모두 반영하지는 못했음을 의미한다. 그러나 첫번째 시나리오인 OCI->M 시나리오에서는 AUC가 2% 미만으로 감소했으며 HTER은 5%보다 조금 넘게 증가했다. 이것은 성능 면에서 조금 높음~높음 사이에 해당하는 지표로써 특정 시나리오에서는 FLIP의 우수한 성능을 일부 따라갔다고 평가할 수 있다. <br><br> 다음으로, FLIP의 성능을 유사하게 가져오는 면에서는 성공률이 미비했지만, 다른 FAS 모델에 비하여 T-FLIP의 성능이 높은지 측정하였다. 기존의 FAS 모델과 비교했을 때는 모든 시나리오에서 T-FLIP의 성능이 대등하거나 더 우수했다. HTER은 T-FLIP이 작게는 2에서부터 크게는 10% 만큼 작았고 주로 다른 모델들과 7% 정도의 차이를 보였다. 이것은 위의 평가 기준에 따라 많이 높은 성능 차이를 의미한다. AUC의 경우 두번째 시나리오에서만 MDDR과 2 이하의 차이를 보였고, 다른 모든 시나리오에서는 작게는 4%에서 높게는 9%에 육박하는 큰 차이가 났다. 이것을 통해서 기존 FAS 모델들과는 많이 높은 성능 차이가 난다는 결론을 낼 수 있다. <br><br> 그리고 특징 기반 지식증류, 관계 기반 지식 증류 두가지의 지식 증류를 FLIP에 적용시켰을 때와 T-FLIP의 성능을 비교해보았다. 그 결과, 관계기반 지식 증류를 적용했을 때보다는 평균 HTER은 6% 낮았고, AUC는 작게는 4%부터 크게는 11%까지 증가하여 위의 평가 기준에 따라 높음에서 매우 높음 정도의 성능 차이를 달성했다. 특징기반 지식 증류에 대해서도 HTER은 평균 4만큼 감소했으며 AUC는 작게는 2%부터 크게는 8% 차이가 났다. 이것 또한 조금 높음에서 많이 높음 만큼의 성능 차이를 보였다. <br><br> 추가적으로, 교사모델의 지식을 학생모델에게 전달한다는 지식 증류의 근본적인 목표를 다른 지식 증류 방식에 비해서 T-FLIP이 성공적으로 달성하고 있는지 확인하기 위해 추가 분석을 진행했다. 특징 기반 지식 증류가 T-FLIP에 비해서는 성능이 낮긴 했지만 비교한 모델들 중에서는 가장 높았기에 두 모델의 비교 분석을 진행하였다. Attention Map을 시각화하는 방식을 통해 T-FLIP과 특징 기반 지식 증류 모델이 교사모델이 주목하고 있는 위변조 탐지 단서와 얼마나 유사하게 주목하는 지를 확인하고자 했다. 결과는 아래와 같다. <img width="1200" alt="스크린샷 2024-12-12 오후 6 07 29" src="https://github.com/user-attachments/assets/257c65ad-0036-4bb2-9e99-4634e0e973f5" /> <br> 결과를 보면 특징 기반 지식 증류 모델은 교사모델과 아예 다른 부분에 집중하고 있음을 알 수 있으며 T-FLIP이 훨씬 교사모델과 유사한 단서에 집중하고 있음을 확인할 수 있다. 이러한 결과는 Attention weight-based KD가 FLIP의 정보를 T-FLIP에게 잘 전달해주고 있음을 나타내고, 이것은 T-FLIP이 FLIP의 높은 일반화 성능을 재현할 수 있는 가능성을 보인다고 해석할 수 있다.|

<br>
 
# Conclusion
| 항목 | 내용 |
|:---  |---  |
| (1) 결론(Conclusion) | 본 프로젝트에서는 지식 증류 기법을 활용하여 FAS 모델인 FLIP-MCL의 크기를 약 70%, 연산량을 약 90% 정도 줄일 수 있었다. 특히 추론 시 교사 모델과 학생 모델이 각 레이어에서 주목하는 특징에 차이가 있다는 점을 관찰하여 양 모델의 어텐션 가중치를 최소화하는 어텐션 가중치 기반 지식 증류 방법을 도입하였고, 그 결과 교사 모델과 학생 모델 간의 성능 격차를 줄일 수 있었다. 결과적으로 전반적으로 다른 FAS 모델 또는 지식 증류 기법에 비해서 대등하거나 우수한 성능을 보였으며, 평가 시나리오 중 하나인 OCI-> M 시나리오에서는 AUC 약 2% 감소만을 이루어내었다. 따라서 FAS 분야에서 모델의 경량화와 성능 유지라는 두가지 목표를 동시에 달성했으며, 이를 통해 보안이 한층 강화된 안면 인식 기능을 컴퓨팅 리소스가 제한된 여러 환경에서도 실시간으로 동작 시켜 해당 기술의 실용성을 높일 수 있을 것으로 기대한다. |

